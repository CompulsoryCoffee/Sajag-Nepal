import numpy as np
import geopandas as gpd
import math
import xarray as xr
import rioxarray as rio
# import dask.array as da
from scipy.stats import lognorm
from tqdm import tqdm, trange
import matplotlib.pyplot as plt
# import scienceplots
import os
# os.environ['USE_PYGEOS'] = '0'
import geopandas as gpd
from numba import njit
# import subprocess

from mods_geom_ops import Pcentroid_Rsampling

# plt.style.use(['science', 'no-latex'])


def Extract_vulnerability_parameters_from(vulnerability):
    # print("manage dictionary and clean up ...")
    # https://github.com/37stu37/hyperedges_paper_2022/blob/main/pre-processing_bldg_w_vulnerability.ipynb?short_path=a4a8fcc
    type_idx = []
    type_cnt = []
    type_size = []

    idx = 0

    # Change string to dictionary
    # print("Extract vulnerability parameters from dictionary ...")
    for (c, s) in zip(vulnerability.cnt_dist, vulnerability.size_dist):

        if c.endswith(('}')):
            d = eval(c)
            type_cnt.append(d)
        else:  # for incomplete dictionary remove invalid part of the string starting from the end
            # print(f"idx = {idx}", c)
            try:
                d = eval(c.rpartition(', "')[0] + ' }')
                type_cnt.append(d)
                
            except Exception as e:
                print(e)
                print(f"idx = {idx}")
                print(f'c = {c}')
                print(f's = {s}')
            
            

        if s.endswith(('}')):
            d = eval(s)
            type_size.append(d)
        else:  # for incomplete dictionary remove invalid part of the string starting from the end
            # print(f"idx = {idx}", s)
            d = eval(s.rpartition(', "')[0] + ' }')
            type_size.append(d)
            idx += 1

        type_idx.append(idx)
        
    

    # Add dictionaries to vulnearbility dataframe
    vulnerability['c_type'] = type_cnt
    vulnerability['c_size'] = type_size

    # extract number of buildings and types
    nbr_bldg = []
    type_bldg = []

    # print('extracting building typologies and counts :')
    for s in type_cnt:
        # d = eval(s)
        nbr_bldg.append(list(s.values()))
        type_bldg.append(list(s.keys()))

    return nbr_bldg, type_bldg


def Calculate_probability_of_collapse_and_fatalities_withcutoff(vulnerabilities, building_occupancy, cutoff):

    # print("Calculate probability of collapse ...")
    
    fragility_data = {
        "C99": {
            "High": lognorm(0.64, scale=0.35).cdf,
            "Mid": lognorm(0.83, scale=1.29).cdf,
            "Low": lognorm(0.71, scale=1.95).cdf,
        },
        "MUR+ADO": {
            "High": lognorm(0.97, scale=0.22).cdf,
            "Mid": lognorm(0.97, scale=0.22).cdf,
            "Low": lognorm(1.73, scale=0.97).cdf,
        },
        "MUR+MOC": {
            "High": lognorm(0.247, scale=0.361).cdf,
            "Mid": lognorm(0.245, scale=0.445).cdf,
            "Low": lognorm(0.93, scale=1.9).cdf,
        },
        "MUR+MOM": {
            "High": lognorm(0.31, scale=0.23).cdf,
            "Mid": lognorm(0.48, scale=0.22).cdf,
            "Low": lognorm(1.96, scale=1.26).cdf,
        },
        "MUR+STRUB": {
            "High": lognorm(0.308, scale=0.203).cdf,
            "Mid": lognorm(0.69, scale=0.39).cdf,
            "Low": lognorm(1.96, scale=1.26).cdf,
        },
        "S": {
            "High": lognorm(0.64, scale=0.3).cdf,
            "Mid": lognorm(0.64, scale=0.38).cdf,
            "Low": lognorm(0.64, scale=0.6).cdf,
        },
        "W": {
            "High": lognorm(0.64, scale=0.77).cdf,
            "Mid": lognorm(0.64, scale=0.95).cdf,
            "Low": lognorm(0.64, scale=1.34).cdf,
        },
    }

    probability_collapse_high = []
    probability_collapse_mid = []
    probability_collapse_low = []
    probability_fatalityRate_high = []
    probability_fatalityRate_mid = []
    probability_fatalityRate_low = []

    # iterate through the vulnerability mixes
    # print("calculate weighted probability of collapse ...")
    for typologies, counts, pga in tqdm(zip(vulnerabilities.types.values,
                                            vulnerabilities.type_counts.values,
                                            vulnerabilities.pgas)):
        mix_collapse_high = []
        mix_collapse_mid = []
        mix_collapse_low = []
        mix_counts = []
        mix_fatalityRate_high = []
        mix_fatalityRate_mid = []
        mix_fatalityRate_low = []            

        for typology, count in zip(typologies, counts):                

            if typology.startswith('C99'):
                H, M, L = fragility_data['C99']["High"](pga), fragility_data['C99']["Mid"](pga), fragility_data['C99'][
                    "Low"](pga)
                # % Fatality from complete damage -> Complete damage * Collapse proba * Fatality Rate (from Robinson et al., 2018)
                H_f, M_f, L_f = H*0.13*0.1*building_occupancy, M*0.13*0.1*building_occupancy, L*0.13*0.1*building_occupancy

            elif (typology.startswith('MUR')):

                if ('ADO' in typology):
                    H, M, L = fragility_data['MUR+ADO']["High"](pga), fragility_data['MUR+ADO']["Mid"](pga), \
                    fragility_data['MUR+ADO']["Low"](pga)
                    H_f, M_f, L_f = H*0.15*0.05*building_occupancy, M*0.15*0.05*building_occupancy, L*0.15*0.05*building_occupancy

                elif ('STRUB' in typology):
                    H, M, L = fragility_data['MUR+STRUB']["High"](pga), fragility_data['MUR+STRUB']["Mid"](pga), \
                    fragility_data['MUR+STRUB']["Low"](pga)
                    H_f, M_f, L_f = H*0.15*0.05*building_occupancy, M*0.15*0.05*building_occupancy, L*0.15*0.05*building_occupancy

                elif ('MOC' in typology):
                    H, M, L = fragility_data['MUR+MOC']["High"](pga), fragility_data['MUR+MOC']["Mid"](pga), \
                    fragility_data['MUR+MOC']["Low"](pga)
                    H_f, M_f, L_f = H*0.15*0.15*building_occupancy, M*0.15*0.15*building_occupancy, L*0.15*0.15*building_occupancy

                elif ('MOM' in typology):
                    H, M, L = fragility_data['MUR+MOM']["High"](pga), fragility_data['MUR+MOM']["Mid"](pga), \
                    fragility_data['MUR+MOM']["Low"](pga)
                    H_f, M_f, L_f = H*0.15*0.05*building_occupancy, M*0.15*0.05*building_occupancy, L*0.15*0.05*building_occupancy
                
                else:
                    H, M, L = fragility_data['MUR+STRUB']["High"](pga), fragility_data['MUR+STRUB']["Mid"](pga), \
                    fragility_data['MUR+STRUB']["Low"](pga)
                    H_f, M_f, L_f = H*0.15*0.05*building_occupancy, M*0.15*0.05*building_occupancy, L*0.15*0.05*building_occupancy

            elif (typology.startswith('S')):
                H, M, L = fragility_data['S']["High"](pga), fragility_data['S']["Mid"](pga), fragility_data['S']["Low"](
                    pga)
                H_f, M_f, L_f = H*0.15*0.1*building_occupancy, M*0.15*0.1*building_occupancy, L*0.15*0.1*building_occupancy

            elif (typology.startswith('W')):
                H, M, L = fragility_data['W']["High"](pga), fragility_data['W']["Mid"](pga), fragility_data['W']["Low"](
                    pga)
                H_f, M_f, L_f = H*0.03*0.005*building_occupancy, M*0.03*0.005*building_occupancy, L*0.03*0.005*building_occupancy

            elif (typology.startswith('MATO')):
                H, M, L = fragility_data['MUR+STRUB']["High"](pga), fragility_data['MUR+STRUB']["Mid"](pga), \
                fragility_data['MUR+STRUB']["Low"](pga)
                H_f, M_f, L_f = H*0.15*0.05*building_occupancy, M*0.15*0.05*building_occupancy, L*0.15*0.05*building_occupancy

            # else:
            #     print(f"problem with {typologies} --> {typology}")

            # append probability cases of collapse for typology
            mix_collapse_high.append(H)
            mix_collapse_mid.append(M)
            mix_collapse_low.append(L)
            mix_fatalityRate_high.append(H_f)
            mix_fatalityRate_mid.append(M_f)
            mix_fatalityRate_low.append(L_f)
            mix_counts.append(count)

        # calculate the weighted probability of collapse
        weighted_high = np.average(mix_collapse_high, weights=mix_counts)
        weighted_mid = np.average(mix_collapse_mid, weights=mix_counts)
        weighted_low = np.average(mix_collapse_low, weights=mix_counts)
        weighted_fatalityRate_high = np.average(mix_fatalityRate_high, weights=mix_counts)
        weighted_fatalityRate_mid = np.average(mix_fatalityRate_mid, weights=mix_counts)
        weighted_fatalityRate_low = np.average(mix_fatalityRate_low, weights=mix_counts)

        # manual correction for fragility curve crossing at low pgas --> NOT NEEDED Alex Densmore 23Oct2023 --> UPDATE create incoherences with plotting the low, mid and high case
        weighted_high = np.maximum(np.maximum(weighted_high, weighted_mid), weighted_low)
        weighted_low = np.minimum(np.minimum(weighted_high, weighted_mid), weighted_low)
        weighted_fatalityRate_high = np.maximum(np.maximum(weighted_fatalityRate_high, weighted_fatalityRate_mid), weighted_fatalityRate_low)
        weighted_fatalityRate_low = np.minimum(np.minimum(weighted_fatalityRate_high, weighted_fatalityRate_mid), weighted_fatalityRate_low)

        # append weighted probability to list
        # force cutoff for low pga
        if pga < cutoff: 
            probability_collapse_high.append(0)
            probability_collapse_mid.append(0)
            probability_collapse_low.append(0)
            probability_fatalityRate_high.append(0)
            probability_fatalityRate_mid.append(0)
            probability_fatalityRate_low.append(0)

        else:
            probability_collapse_high.append(weighted_high)
            probability_collapse_mid.append(weighted_mid)
            probability_collapse_low.append(weighted_low)
            probability_fatalityRate_high.append(weighted_fatalityRate_high)
            probability_fatalityRate_mid.append(weighted_fatalityRate_mid)
            probability_fatalityRate_low.append(weighted_fatalityRate_low)

    # attach to vulnerability layer
    vulnerabilities['collapse_high'] = probability_collapse_high
    vulnerabilities['collapse_mid'] = probability_collapse_mid
    vulnerabilities['collapse_low'] = probability_collapse_low
    vulnerabilities['fatalityRate_high'] = probability_fatalityRate_high
    vulnerabilities['fatalityRate_mid'] = probability_fatalityRate_mid
    vulnerabilities['fatalityRate_low'] = probability_fatalityRate_low

    return vulnerabilities[["collapse_high", "collapse_mid", "collapse_low", "fatalityRate_high", "fatalityRate_mid", "fatalityRate_low", "pgas", "geometry"]]


def Join_buildings_vulnerability(buildings, vulnerabilities):
    joined = gpd.sjoin_nearest(buildings, vulnerabilities, how="left", distance_col="distances")
    buildings = joined.sort_values(by=['distances']).drop_duplicates(subset=['osm_id'], keep="first")
    del joined
    return buildings


def plot_data_geometry_by_attribute(data, shake_ras):

    # Plotting the vulnerabilities on a map
    fig, axs = plt.subplots(1, 3, figsize=(15, 5))

    # Plotting collapse_high
    vmin = 0.0  # Minimum value
    vmax = 1.0  # Maximum value

    im0 = axs[0].scatter(data.centroid.x, data.centroid.y, marker='s', s=1.0, c=data.collapse_high, vmin=vmin, vmax=vmax, cmap="viridis")
    axs[0].set_title('Collapse High')
    fig.colorbar(im0)

    im1 = axs[1].scatter(data.centroid.x, data.centroid.y, marker='s', s=1.0, c=data.collapse_mid, vmin=vmin, vmax=vmax, cmap="viridis")
    axs[1].set_title('Collapse Mid')
    fig.colorbar(im1)

    im2 = axs[2].scatter(data.centroid.x, data.centroid.y, marker='s', s=1.0, c=data.collapse_low, vmin=vmin, vmax=vmax, cmap="viridis")
    axs[2].set_title('Collapse Low')
    fig.colorbar(im2)

    # Customize the subplots
    for ax in axs:
        ax.set_xlabel('Longitude')
        ax.set_ylabel('Latitude')

    # Adjust the spacing between subplots
    plt.tight_layout()

    # Display the plot
    plt.savefig(f'/Volumes/prs/ensemble_data/results/png/{shake_ras[8:-4]}__eqImpact_DASK_PLOTS.png', dpi=300)
    # plt.show()


@njit()
def mh_cascade_scenario(EXP_IDS, PPGA_SU, 
                        SI_SLOPE_MEAN, SI_SLOPE_STD, 
                        SI_BLDG_MEAN, SI_BLDG_STD, 
                        FLOWR_BLDG_MEAN, FLOWR_BLDG_STD):
    
    a = np.zeros(EXP_IDS.shape[0], np.float64)
    b = np.zeros(EXP_IDS.shape[0], np.float64)

    for i, _ in enumerate(EXP_IDS):
        rng = np.random.uniform(0, 1)
        if PPGA_SU[i] > rng:
            rng = np.random.uniform(0, 1)
            if np.random.normal(SI_SLOPE_MEAN[i], SI_SLOPE_STD[i]) > rng: # if slope generate landslide
                rng = np.random.uniform(0, 1)
                if np.random.normal(SI_BLDG_MEAN[i], SI_BLDG_STD[i]) > rng: # if asset is impacted by landslide
                    a[i] = 1 # immediate impact
                rng = np.random.uniform(0, 1)
                if np.random.normal(FLOWR_BLDG_MEAN[i], FLOWR_BLDG_STD[i]) > rng: # if flowr runout  impact
                    b[i] = 1 # long term impact
    return a, b


def landslide_impact(data_dir, shake_ras, shake_dir, polygons, polygons_name, n_scenarios, resdir):

    from scipy.stats import norm
    import pandas as pd
    from mods_geom_ops import Pcentroid_Rsampling, join_Pcentroid_at_Pl
    
    slopes = gpd.read_file(data_dir / "su_updt.shp")
    # Rename columns to avoid duplication when merging
    new_column_names = {'xgb_mean': 'mean_slp', 'xgb_std': 'std_slp'}
    slopes.rename(columns=new_column_names, inplace=True)
    slopes = slopes[["su_id", "mean_slp", "std_slp", "geometry"]]
    
    if polygons_name == "buildings":
        fR_stats = pd.read_csv(data_dir / "bldgs_fR_stats.csv")
        fR_stats = fR_stats[["osm_id", "FlowR_mean", "FlowR_std"]]
        
    if polygons_name == "roads":
        fR_stats = pd.read_csv(data_dir / "roads_fR_stats_2m_join.csv")
        fR_stats = fR_stats[["osm_id", "FlowR_mean", "FlowR_std"]]


    # sample PGA values at slope unit
    slopes_pga = Pcentroid_Rsampling(slopes, shake_ras, shake_dir)

    # probability of slope triggering landslide from specific PGA
    # parameters from best fit with true positive from Gorkha
    # from eq induced_landslide_gorkha_parameters.py
    μ = 0.5064317
    σ = 0.12733273
    slopes_pga["pPGA_su"] = norm(μ, scale=σ).cdf(slopes_pga['pgas'])

    # slope attributes to building dataset
    polygons = join_Pcentroid_at_Pl(polygons,
                                 ["osm_id", "MEAN", "STD", "geometry"],
                                 slopes_pga,
                                 ["su_id", "mean_slp", "std_slp", "pPGA_su", "geometry"])

    # add FlowR values to osm
    polygons = pd.merge(polygons, fR_stats, on="osm_id", how="left")
    del slopes, fR_stats

    # generate landslide scenarios for the earthquake
    scenarios_results = np.zeros(polygons.osm_id.values.shape[0], np.float64)
    scenarios_results_long = np.zeros(polygons.osm_id.values.shape[0], np.float64)

    # Monte Carlo simulation of landslides for earthquake
    # print(f"running {n_scenarios} landslide scenarios")
    # print("")
    for n in trange(n_scenarios):
        scenario_impact, scenario_impact_long = mh_cascade_scenario(EXP_IDS=polygons.osm_id.values,
                                              PPGA_SU=polygons.pPGA_su.values,
                                              SI_SLOPE_MEAN=polygons.mean_slp.values,
                                              SI_SLOPE_STD=polygons.std_slp.values,
                                              SI_BLDG_MEAN=polygons.MEAN.values,
                                              SI_BLDG_STD=polygons.STD.values,
                                              FLOWR_BLDG_MEAN=polygons.FlowR_mean.values,
                                              FLOWR_BLDG_STD=polygons.FlowR_std.values)

        scenarios_results = scenarios_results + scenario_impact
        scenarios_results_long = scenarios_results_long + scenario_impact_long

    polygons['impact'] = scenarios_results / n_scenarios
    polygons['impact_long'] = scenarios_results_long / n_scenarios
    polygons['event'] = shake_ras[14:-4]
    
    # print(f"{polygons_name} is being saved to disk")
    polygons.to_parquet(resdir / f'{shake_ras[8:-4]}_{polygons_name}_lsImpact.parquet', index=False)
    del polygons, scenarios_results
    
    return


def earthquake_impact(data_dir, shake_ras, shake_dir, road_only, resdir, building_occupancy):
    
    if road_only == "no":
      
        # Load data
        # buildings = dgpd.read_parquet(data_dir / "bldgs.parquet", npartitions=2)
        # buildings = gpd.read_file(data_dir / "bldgs_light.gpkg")
        # buildings = buildings[["osm_id", "DISTRICT", "MEAN", "STD", "geometry"]]#.compute()
        buildings = gpd.read_parquet(data_dir / "bldgs_light.parquet")
    
        vulnerability = gpd.read_file(data_dir / "npl-ic-exp-dist_UTM45.shp")
        vulnerability = vulnerability[["size_dist", "cnt_dist", "geometry"]]
    
        # print("\n datasets loaded")
        vulnerability['type_counts'], vulnerability['types'] = Extract_vulnerability_parameters_from(vulnerability)
        vulnerability = vulnerability[["types", "type_counts", "geometry"]]
    
        print(f"{shake_ras} Started")
        
        # extract pga at METEOR data point
        vulnerability = Pcentroid_Rsampling(vulnerability, shake_ras, shake_dir)
    
        # attach probability of collapse to each building
        vulnerability = Calculate_probability_of_collapse_and_fatalities_withcutoff(vulnerability, building_occupancy=building_occupancy, cutoff=0.1)
    
        # join vulnerabilities to buildings
        buildings = Join_buildings_vulnerability(buildings, vulnerability)
        
        del vulnerability
        
        # calculate cascading impact (landslides)
        landslide_impact(data_dir, shake_ras, shake_dir, polygons=buildings, polygons_name="buildings", n_scenarios=10000, resdir=resdir)
    
        # write to file
        buildings['event'] = shake_ras[14:-4]
        buildings.to_parquet(resdir / f'{shake_ras[8:-4]}_buildings_eqImpact.parquet', index=False)
        
        del buildings
    
    
    # Load road data
    roads = gpd.read_parquet(data_dir / "roads_light.parquet")
    # roads = gpd.read_file(data_dir / "roads.parquet")
    # roads = gpd.read_file(data_dir / "roads_light.gpkg")
    # roads = roads[["osm_id", "MEAN", "STD", "geometry"]].compute()
    
    # calculate cascading impact (landslides)
    landslide_impact(data_dir, shake_ras, shake_dir, polygons=roads, polygons_name="roads", n_scenarios=10000, resdir=resdir)

    del roads
    
    return print(f"{shake_ras} Done")
